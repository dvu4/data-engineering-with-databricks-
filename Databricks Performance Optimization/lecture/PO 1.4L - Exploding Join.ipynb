{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d25b6e5-63b2-4f3a-967c-71abb58596f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bebaf5da-b3ab-4ddb-9f4e-277f24cccdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Exploding Join\n",
    "\n",
    "In this lab, we will be working on improving query performance on an exploding join between 3 tables:\n",
    "- **transactions**\n",
    "- **stores**\n",
    "- **countries**\n",
    "\n",
    "We want to introduce and identify spill introduced by the exploding join and gradually improve the performance of the join.\n",
    "\n",
    "#### Lab Notes\n",
    "To successfully create a exploding join that spills to disk we had to specifically turn off the following optimizations:\n",
    "- [Predictive optimization](https://docs.databricks.com/aws/en/optimizations/predictive-optimization) for Unity Catalog managed tables in the **lab** schema.\n",
    "- Broadcast joins to gradually demonstrate the performance improvements of tuning the join strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de476d9-0f3f-4b2f-bc83-3fde5f9495a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. If you use Serverless, errors will be returned when setting compute runtime properties.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be8f8d09-f1f4-4393-9660-0de7a757a489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to your unique **labuser** catalog, and the default schema to **lab**. All tables will be read from and written to this location.\n",
    "<br></br>\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "691b115a-aa0f-4e4e-8820-cfb4acab35af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-4L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31623c86-2013-497e-949b-21e48cdf27f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Disable Caching\n",
    "\n",
    "Run the following cell to set a Spark configuration variable that disables disk caching.\n",
    "\n",
    "Turning disk caching off prevents Databricks from storing cloud storage files after the first query. This makes the effect of the optimizations more apparent by ensuring that files are always pulled from cloud storage for each query.\n",
    "\n",
    "For more information, see [Optimize performance with caching on Databricks](https://docs.databricks.com/en/optimizations/disk-cache.html#optimize-performance-with-caching-on-databricks).\n",
    "\n",
    "**NOTE:** This will not work in Serverless. Please use classic compute to turn off caching. If you're using Serverless, an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9f760e-e8ad-40bd-b8df-21c42ceeee76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.io.cache.enabled', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce792900-222c-453c-b87f-7c3a3e614056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Creating & Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4788645-5aaf-4e44-a15e-67ee9bb16865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Create the transactions table\n",
    "Generate the **transactions** table with the schema below and write it to a Delta table. This is the table with the largest amount of data, containing 2,000,000 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c7a22a-4cfe-4c3e-92bf-28f3e94322bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "## Delete the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS transactions')\n",
    "\n",
    "\n",
    "## Create the table\n",
    "transactions_df = (spark\n",
    "                   .range(0, 2000000, 1, 32)\n",
    "                    .select(\n",
    "                        'id',\n",
    "                        round(rand() * 10000, 2).alias('amount'),\n",
    "                        (col('id') % 10).alias('country_id'),\n",
    "                        (col('id') % 100).alias('store_id')\n",
    "                    )\n",
    "                    .write\n",
    "                    .mode('overwrite')\n",
    "                    .option(\"overwriteSchema\", \"true\")\n",
    "                    .saveAsTable('transactions')\n",
    "                )\n",
    "\n",
    "## Display the table\n",
    "display(spark.sql('SELECT * FROM transactions LIMIT 10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5e9b692-9f44-4689-8241-1360f36cfaf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Create the countries table\n",
    "Generate the **countries** table to join with the **transactions** table later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6974ff60-c565-4d37-b21c-83570bbc2f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Drop the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS countries')\n",
    "\n",
    "## Create the table\n",
    "countries = [(0, \"Italy\"),\n",
    "             (1, \"Canada\"),\n",
    "             (2, \"Mexico\"),\n",
    "             (3, \"China\"),\n",
    "             (4, \"Germany\"),\n",
    "             (5, \"UK\"),\n",
    "             (6, \"Japan\"),\n",
    "             (7, \"Korea\"),\n",
    "             (8, \"Australia\"),\n",
    "             (9, \"France\"),\n",
    "             (10, \"Spain\"),\n",
    "             (11, \"USA\")\n",
    "            ]\n",
    "columns = [\"id\", \"name\"]\n",
    "\n",
    "countries_df = (spark\n",
    "                .createDataFrame(data = countries, schema = columns)\n",
    "                .write\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable(\"countries\")\n",
    "            )\n",
    "\n",
    "\n",
    "## Display the table\n",
    "display(spark.sql('SELECT * FROM countries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ffaf5a-6c13-4dd4-a399-7a60c7625f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C3. Create the stores table\n",
    "- Generate the **stores** table to join with the **transactions** table later on.\n",
    "- The **stores** table intentionally contains duplicates of the **id** value, which will introduce an exploding join with the **transactions** table.\n",
    "- When joining the **transactions** table with the **stores** table, a number of rows will explode due to the duplicated **ids**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5552c93b-f834-4f94-9859-59b090a4346d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Drop the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS stores')\n",
    "\n",
    "\n",
    "stores_df = (spark\n",
    "                .range(0, 9999)\n",
    "                .select(\n",
    "                    (col('id') % 100).alias('id'), # intentionally duplicating ids to explode the join\n",
    "                    round(rand() * 100, 0).alias('employees'),\n",
    "                    (col('id') % 10).alias('country_id'),\n",
    "                    expr('uuid()').alias('name')\n",
    "                )\n",
    "                .write\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable('stores')\n",
    "            )\n",
    "\n",
    "\n",
    "## Display the table\n",
    "display(spark.sql('SELECT * FROM stores ORDER BY id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbec835c-2e15-40f0-83ec-77d1c2d09591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Perform an Exploding Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc62fd3b-4208-4f88-8a9f-27287576016f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this cell, turn off broadcast joins to gradually demonstrate the performance improvements of tuning the join strategy.\n",
    "\n",
    "\n",
    "- [spark.sql.autoBroadcastJoinThreshold](https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html#other-configuration-options) documentation\n",
    "\n",
    "- [spark.databricks.adaptive.autoBroadcastJoinThreshold](https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html#converting-sort-merge-join-to-broadcast-join) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41a1047-287d-4579-80a3-8c74b5c5ae15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Disabling the automatic broadcast join entirely. That is, Spark will never broadcast any dataset for joins, regardless of its size.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Disabling the broadcast join feature under AQE, meaning that even when using adaptive query execution, Spark will not attempt to broadcast any smaller side of a join.\n",
    "spark.conf.set(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb83c5c4-3ec5-4fcc-be84-832bd3c661e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will be joining the **transactions** table with the **countries** and **stores** data, and trigger the action by saving the result to a table named **transact_countries**. The query has been written for you below.\n",
    "\n",
    "Run the cell, note down the time taken to execute the query, and compare it with each optimization.\n",
    "\n",
    "**NOTE:** This should take about ~1 minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ffc072b-8770-4d7f-b9fe-303a33079094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df_nobroadcast = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "    JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "\"\"\")\n",
    "\n",
    "(joined_df_nobroadcast\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b90d523-2238-4cc4-8f82-bfc56e94f941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. TODO: See the Exploding Join\n",
    "Open the Spark UI and navigate to the **Stages** page. Identify the explosion of rows in the DAG of the Spark UI. To view the DAG, complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. In the first job , right-click on **View** and select *Open in a New Tab*. \n",
    "    - **If the first job does not display the correct information, try the second one**.\n",
    "\n",
    "    **NOTES:** In the Vocareum lab environment, if you click **View** without opening it in a new tab, the pop-up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here, you should see the entire query plan. Read the DAG graph from bottom to top to better understand the details of the execution plan.\n",
    "\n",
    "<br></br>\n",
    "![1.4-d1-exploding_join_shuffle_dag.png](./Includes/images/1.4-d1-exploding_join_shuffle_dag.png)\n",
    "\n",
    "5. In the query plan, below the number of rows (199,980,000), expand the **PhotonShuffleExchangeSink** box (the arrow in the image above shows you what to expand). Notice that the **metric** for *estimated rows output* has exploded by 100x after joining the **transactions** table with the **stores** table, which is caused by duplicates of the store-id in the stores table.\n",
    "\n",
    "6. In the same location, look at the **metric** **num bytes spilled to disk due to memory pressure total (min, med, max)**. Notice that *927.6 MiB* (value can vary) spilled to disk.\n",
    "\n",
    "7. Leave the Spark UI open.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74433ace-a643-4db3-ba22-8021e47dc3b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. TODO: See the Spill\n",
    "In the Spark UI view the memory spill. \n",
    "\n",
    "1. In the Spark UI select **Stages** on the top navigation bar. Here you can see all the stages performed on the cluster.\n",
    "\n",
    "2. Find the stage with the largest amount of **Shuffle Writes** (should be around 1335.0 MiB, but it can vary) for the query within the **Description** column that begins with `joined_df_nobroadcast = spark(\"\"\"SELECT...)`.\n",
    "\n",
    "3. After you find that stage, select the link in the **Description** field.\n",
    "![1.4-d2_find_spill.png](./Includes/images/1.4-d2_find_spill.png)\n",
    "\n",
    "4. Look at the field **Spill (Disk)** for the stage. Notice that this query spilled to disk.\n",
    "\n",
    "    **NOTE:** In Apache Spark, when there's more data than it can handle in memory, it puts some of the extra data on the hard drive. This is called *spilling to disk* and the 927.9 MiB means it had to move about 927.9 megabytes of data to the hard drive to keep processing.\n",
    "\n",
    "<br></br>\n",
    "![1.4-d2_memory_spill.png](./Includes/images/1.4-d2_memory_spill.png)\n",
    "\n",
    "\n",
    "\n",
    "5. In the same page look at the **Locality Level Summary**. This means that the number of partitions in this stage is 4. Think to yourself, is this a good setting?\n",
    "\n",
    "6. Close the Spark UI browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd8461d-220a-4942-8792-5ad85731bd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## E. Improve: Increasing the number of shuffle partitions\n",
    "Let's try to increase the performance of this query by increasing the number of shuffle partitions. You can do this by modifying the **spark.sql.shuffle.partitions** configuration setting and setting it to **8** partitions. \n",
    "\n",
    "This configures the number of partitions to use when shuffling data for joins or aggregations.\n",
    "\n",
    "For more information, view the [spark.sql.shuffle.partitions](https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html#other-configuration-options) documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8fc2d6-7eb9-4958-8b04-3b7ed218534c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Rerun the cell to turn off the broadcast join features if not already turned off\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "## Deal with the spill by increasing the number of shuffle partitions to 8\n",
    "## Not a huge difference in this small example, but as the amount of data spilled increases\n",
    "## This can make a big difference\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", <FILL_IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb1e862-4880-4a69-be06-ca6450984a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the same query as the previous example, but this time with the number of shuffle partitions set to 8. Note the time the query takes to execute.\n",
    "\n",
    "**NOTE:** This should take about ~50 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f559a80-1ee4-46c7-9775-590075784b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df_8_partitions = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "    JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "\"\"\")\n",
    "\n",
    "(joined_df_8_partitions\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")\n",
    "\n",
    "## Reset the shuffle.partions to the default setting\n",
    "spark.conf.unset(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ced5c23-43cc-40a1-9a45-266138960614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. TODO: See the Spill\n",
    "\n",
    "In the Spark UI view the memory spill. \n",
    "\n",
    "1. Expand **Spark Jobs**, on the first job right click and select **Open in a New Tab**.\n",
    "\n",
    "2. In the Spark UI select **Stages** on the top navigation bar.\n",
    "\n",
    "3. Find the stage with the largest amount of shuffle writes (should be around 679.9 MiB, but it can vary) for the query within the **Description** column that begins with `joined_df_8_partitions = spark(\"\"\"SELECT...)`.\n",
    "\n",
    "4. After you find that stage, select the link in the **Description** field.\n",
    "\n",
    "5. Look at the field **Spill (Disk)**. Notice that this stage spilled to disk.\n",
    "\n",
    "    **NOTE:** In Apache Spark, when there's more data than it can handle in memory, it puts some of the extra data on the hard drive. This is called *spilling to disk* and the 273.4 MiB means it had to move about 273.4 MiB of data to the hard drive to keep processing.\n",
    "\n",
    "    When modifying the number of partitions the spill has decreased.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "![1.4-e_memory_spill.png](./Includes/images/1.4-e_memory_spill.png)\n",
    "<br></br>\n",
    "\n",
    "6. In the same page look at the **Locality Level Summary**. This means that the number of partitions in this stage is 6 even though we set the number of partitions to 8. This is because Spark automatically decides to combine smaller partitions into bigger ones during a job, which can help Spark finish the job more quickly and use less memory. \n",
    "\n",
    "    You can disable this by setting the following option `spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)`.\n",
    "\n",
    "\n",
    "7. Note down amount of data spilled has been reduced and time taken to execute the query is a bit faster. \n",
    "\n",
    "**NOTE:** The spill metrics of 273MB spilled can be observed in query plan in **PhotonShuffleExchangeSink** that follows the join operations, as well as stage details for stage that has largest amount of shuffle writes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2573d359-e4ba-4eff-9078-8ed9d58b094b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Improve: Change the Order of the Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a2d0e8-45d7-4c3b-8993-38fdbf63b89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to turn off **autoBroadcastJoinThreshold** if not already turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88c41a9c-5598-43f7-97fd-4bfe7a40b19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Turn off the broadcast join features if not already turned off\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93540548-5cfd-4ea6-aaa5-8b37b96dd6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's look at the modified query below, which now joins the **transactions** table with the **countries** first, then joins with the **stores** table. \n",
    "\n",
    "Running the smaller join first (avoiding the large exploding join) means we don't have to shuffle as much data as we did when joining with the **stores** table first.\n",
    "\n",
    "Run the cell and note the amount of time it takes to complete the query.\n",
    "\n",
    "**NOTE:** The query has been modified for you. Simply run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a0bc60-4d13-4359-9d7c-235afafc2ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_joined_first_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    -- Notice that we are joining with countries first to avoid instead of stores --\n",
    "    JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "    -- Then we join the results with the stores table, avoiding the shuffle of the large exploding join  --\n",
    "    JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "(small_joined_first_df\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8cb5b5c-9aaa-4a7a-8b88-83767333d2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F1. TODO: See the Shuffles\n",
    "Open the Spark UI and navigate to the query DAG. Identify the explosion of rows in the DAG of the Spark UI. To view how the DAG works, complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. In the first Job, right-click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment, if you click **View** without opening it in a new tab, the pop-up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here, you should see the entire query plan. Read the DAG graph bottom-up to better understand the details of the execution plan. \n",
    "\n",
    "5. In the query plan view, notice that the first join is between the **transactions** and **countries** tables, which returns 2 million results. Then, the results join with the **stores** table, creating the exploding join. This avoids having to shuffle the large join between **transactions** and **stores** (~200,000,000 rows) as we did in the first query.\n",
    "\n",
    "<br></br>\n",
    "![1.4-f_smaller_join_first_dag.png](./Includes/images/1.4-f_smaller_join_first_dag.png)\n",
    "\n",
    "<br></br>\n",
    "6. Leave the Spark UI open.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f906b373-d28e-4425-8841-61c9d8ae8ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F2. TODO: See the Spill\n",
    "In the Spark UI view the memory spill. \n",
    "\n",
    "1. In the Spark UI select **Stages** on the top navigation bar. Here you can see all the stages performed on the cluster.\n",
    "\n",
    "2. Find the stage with the largest amount of **Shuffle Writes** (should be around 19.1 MiB, but it can vary) for the query within the **Description** column that begins with `small_joined_first_df = spark(\"\"\"SELECT...)`.\n",
    "\n",
    "3. After you find that stage, select the link in the **Description** field.\n",
    "\n",
    "4. Look at the field **Spill (Disk)**. Notice that this query did not spill to disk.\n",
    "\n",
    "<br></br>\n",
    "![1.4-f2_memory_spill.png](./Includes/images/1.4-f2_memory_spill.png)\n",
    "<br></br>\n",
    "\n",
    "\n",
    "5. Close the Spark UI browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59bb82b-93de-48a5-8c2d-ecdeb1d2102c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Can you still see Spill in Spark UI? Did this query run faster or slower than the previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0c38055-e43b-4d20-8ce7-e7a9702cf3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## G. Improve: Analyze and Use Default Broadcast Configurations: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47ebddf8-3191-4462-9e53-906d4edd5a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reset the **spark.sql.autoBroadcastJoinThreshold** and **spark.databricks.adaptive.autoBroadcastJoinThreshold** configurations using the `spark.conf.unset()` method.\n",
    "\n",
    "[spark.sql.autoBroadcastJoinThreshold](https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html#other-configuration-options) documentation\n",
    "\n",
    "[spark.databricks.adaptive.autoBroadcastJoinThreshold](https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html#converting-sort-merge-join-to-broadcast-join) documentation\n",
    "\n",
    "Run the cell and view the results. Confirm the following default values for the configurations:\n",
    "- *Default value of autoBroadcastJoinThreshold: 10,485,760 bytes*\n",
    "- *Default value of adaptive.autoBroadcastJoinThreshold: 31,457,280 bytes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed89c694-0547-4471-8a26-32711ad32fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reset defaults here\n",
    "spark.conf.unset(<FILL_IN>)\n",
    "spark.conf.unset(<FILL_IN>)\n",
    "\n",
    "# Display the values\n",
    "print(f'Default value of autoBroadcastJoinThreshold: {spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")}')\n",
    "print(f'Default value of adaptive.autoBroadcastJoinThreshold: {spark.conf.get(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "322414e6-8f95-4d00-a465-969a40eb87f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cost-based optimizers rely on statistics information to generate the most efficient physical query plan with the lowest cost. This includes decisions on join strategy and the order of joins.\n",
    "\n",
    "Running `ANALYZE` on the joining columns across the three tables allows the optimizer to make better decisions, and everything will work as if by magic. \n",
    "\n",
    "Complete the cell below by writing the required `ANALYZE` statements. \n",
    "\n",
    "**HINT:** View the [ANALYZE TABLE](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-analyze-table.html) documentation for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d59577fa-73d0-4819-8ba1-2e357411a72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analyze the transactions table and compute statistics for columns country_id and store_id\n",
    "\n",
    "\n",
    "# Analyze the stores table and compute statistics for the column id\n",
    "\n",
    "\n",
    "# Analyze the countries table and compute statistics for the column id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67dd5c9-cea0-47c1-9dd7-00d11a1fc936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In the example below, a developer writes joins without considering the optimal order of joins.\n",
    "\n",
    "Run the query again using the initial query we used in this demonstration that joins **transactions** with **stores** first and explodes the data for the large shuffle. \n",
    "\n",
    "Will letting Spark figure out how to join the data efficiently work? \n",
    "\n",
    "Note down time taken to do the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1f7898-56b0-417e-8584-ae9b43d579a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df_analyze = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "    JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "\"\"\")\n",
    "\n",
    "(joined_df_analyze\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b103f7c-6ac5-4561-b331-a6255044b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTES:**\n",
    "- We put the shuffle settings back to the defaults.  Usually best to stick with the defaults which tend to improve over time.  If you hard-code configurations, you may be opting out of future performance improvements unwittingly.  Always worth checking old configurations to make sure they're still needed.  You could get big performance improvements by cleaning out old configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9beb6f7-9259-48e4-b746-0e124277bfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View the Spark UI. Think about the following:\n",
    "- What can you see in DAG of query plan? \n",
    "- Any spill? \n",
    "- Rows exploded? \n",
    "- Order of Join? \n",
    "- Is the DAG similar to the previous joins? \n",
    "- Did the query execute faster? What were the size of the shuffle writes? \n",
    "- Larger or smaller than the previous queries?\n",
    "\n",
    "<br></br>\n",
    "**Stages**\n",
    "- Notice the small amount of shuffle writes.\n",
    "![1.4-g_analyze_stages.png](./Includes/images/1.4-g_analyze_stages.png)\n",
    "\n",
    "<br></br>\n",
    "**DAG**\n",
    "- Look at the differences in the DAG.\n",
    "![1.4_g_dag.png](./Includes/images/1.4_g_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370faae0-c52d-40f2-8adc-105c769b1f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary of the 4 Joins\n",
    "\n",
    "| Join Strategy | Execution Time | Memory Spill | Largest Shuffle Write | Notes |\n",
    "|---------------|----------------|----------------|----------------|----------------|\n",
    "|D. Exploding Join| ~60 seconds| ~928 MiB |\t1334.8 MiB |Joining the **transactions** table with the **store** table (exploding join) first |\n",
    "|E. Increase the number of shuffles| ~50 seconds | ~273.7 MiB | 680.5 MiB |Same join as before, specify 8 partitions |\n",
    "|F. Change the join order | ~40 seconds| 0 | 19.1 MiB |Change the join order to join **transactions** with **countries** first |\n",
    "|G. Analyze and broadcast join| ~20 seconds| 0 | 383.6 KiB |Let Databricks analyze and use default broadcast configurations. While the query took about the same time, |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7754f943-a4a4-46f7-b4bc-ab706bf0061a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "PO 1.4L - Exploding Join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}